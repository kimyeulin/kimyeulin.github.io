{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nextSentencePred.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimyeulin/kimyeulin.github.io/blob/master/nextSentencePred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gkts2lSqFe2",
        "colab_type": "text"
      },
      "source": [
        "# 세팅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tS6Zv-xpfpk",
        "colab_type": "text"
      },
      "source": [
        "#### Google Cloud Platform TPU 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6cUs9uOp5zT",
        "colab_type": "code",
        "outputId": "d01ee875-8c0b-406f-8877-2190d04e8f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.121.67.162:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEEbvoTUq6bY",
        "colab_type": "text"
      },
      "source": [
        "#### 기타 라이브러리 import 및 BERT 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7mcdx-yQ0NX",
        "colab_type": "code",
        "outputId": "963a84a4-2566-4f77-9d6c-0089ce38c4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0607 05:51:42.561981 140496214189952 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Ela1i-qvjx",
        "colab_type": "code",
        "outputId": "5741d008-0ea0-4cc1-e014-796039cc5ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9_Fc68Aq2cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78J-LjVlqFGL",
        "colab_type": "text"
      },
      "source": [
        "#### 저장 디렉토리 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZCQgf-gJwG",
        "colab_type": "text"
      },
      "source": [
        "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
        "\n",
        "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
        "\n",
        "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. ***Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist).***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_EAnICvP7f",
        "colab_type": "code",
        "outputId": "c27defca-6e3e-471a-d31d-34544e29cde6",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Set the output directory for saving model file\n",
        "# Optionally, set a GCP bucket location\n",
        "\n",
        "OUTPUT_DIR = 'nextSentence'#@param {type:\"string\"}\n",
        "#@markdown Whether or not to clear/delete the directory and create a new one\n",
        "DO_DELETE = False #@param {type:\"boolean\"}\n",
        "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
        "USE_BUCKET = True #@param {type:\"boolean\"}\n",
        "BUCKET = 'bert_open_1' #@param {type:\"string\"}\n",
        "\n",
        "if USE_BUCKET:\n",
        "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "if DO_DELETE:\n",
        "  try:\n",
        "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
        "  except:\n",
        "    # Doesn't matter if the directory didn't exist\n",
        "    pass\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://bert_open_1/nextSentence *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtKAmlfE7IS1",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 불러오기 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvXsOtUZQ38O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzARIhRghux",
        "colab_type": "text"
      },
      "source": [
        "#### 데이터 불러오기 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKty0OKzVZc5",
        "colab_type": "text"
      },
      "source": [
        "Task #2: Next Sentence Prediction (NSP)\n",
        "Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order\n",
        "to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically,\n",
        "when choosing the sentences A and B for each pretraining example, ***50% of the time B is the actual\n",
        "next sentence that follows A (labeled as IsNext),\n",
        "and 50% of the time it is a random sentence from\n",
        "the corpus (labeled as NotNext).*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a5X1NpAXn9h",
        "colab_type": "code",
        "outputId": "d5caee61-73a7-46af-d4b3-8be49c4e48db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "'''\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "'''\n",
        "################################################################################\n",
        "################################################################################\n",
        "def load_directory_data(directory):\n",
        "    dataset = pd.read_csv(directory)\n",
        "    types      = dataset[\"유형\"]\n",
        "    paragraphs = dataset[\"지문\"]\n",
        "    answers    = dataset[\"답\"]\n",
        "    \n",
        "    dataset_ = []\n",
        "    for type, paragraph, answer in zip(types,paragraphs,answers):\n",
        "        paragraph = [sentence.strip() for sentence in paragraph.split('.')]\n",
        "        # paragraph = [sentence for sentence in sentences if not sentence =='']\n",
        "        data_ = {}\n",
        "        data_[\"type\"]      = type\n",
        "        data_[\"paragraph\"] = paragraph\n",
        "        data_[\"answer\"]    = answer\n",
        "        dataset_.append(data_)\n",
        "        \n",
        "    type         = []\n",
        "    prevSentence = []\n",
        "    nextSentence = []\n",
        "    sentenceNum  = []\n",
        "    sentenceMax  = []\n",
        "    answer       = []\n",
        "    for data_ in dataset_:\n",
        "        prev = 0 \n",
        "        next = 1\n",
        "        sentenceMax_=len(data_[\"paragraph\"])\n",
        "        while next < sentenceMax_:\n",
        "            type.append(data_[\"type\"])\n",
        "            prevSentence.append(data_[\"paragraph\"][prev])\n",
        "            nextSentence.append(data_[\"paragraph\"][next])\n",
        "            sentenceNum.append(next)\n",
        "            sentenceMax.append(sentenceMax_-1)\n",
        "            answer.append(data_[\"answer\"])#정답\n",
        "            prev=prev+1 \n",
        "            next=next+1\n",
        "\n",
        "    data = {}\n",
        "    data[\"type\"]         = type\n",
        "    data[\"prevSentence\"] = prevSentence\n",
        "    data[\"nextSentence\"] = nextSentence\n",
        "    data[\"sentenceNum\"]  = sentenceNum\n",
        "    data[\"sentenceMax\"]  = sentenceMax\n",
        "    data[\"answer\"]       = answer\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "################################################################################\n",
        "################################################################################\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "'''\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "'''\n",
        "################################################################################\n",
        "################################################################################\n",
        "def load_dataset(directory): # 수정 필요\n",
        "  dataset = load_directory_data(directory)\n",
        "  dataset_t, dataset_f = train_test_split(dataset, test_size=0.5)\n",
        "  nextSentence_rand = list(\n",
        "                          dataset[ dataset[\"nextSentence\"] != '' ]\n",
        "                          [\"nextSentence\"]\n",
        "                          .sample( len(dataset_f) )\n",
        "                          )\n",
        "  del dataset_f[\"nextSentence\"]\n",
        "  dataset_f[\"nextSentence\"] = nextSentence_rand\n",
        "  dataset_t[\"tf\"] = 1\n",
        "  dataset_f[\"tf\"] = 0 \n",
        "  return pd.concat([dataset_t, dataset_f]).sample(frac=1).reset_index(drop=True)\n",
        "  \n",
        "\n",
        "  \n",
        "################################################################################\n",
        "################################################################################\n",
        "# Download and process the dataset files.\n",
        "'''\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef download_and_load_datasets(force_download=False):\\n  dataset = tf.keras.utils.get_file(\\n      fname=\"aclImdb.tar.gz\", \\n      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \\n      extract=True)\\n  \\n  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \\n                                       \"aclImdb\", \"train\"))\\n  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \\n                                      \"aclImdb\", \"test\"))\\n  \\n  return train_df, test_df\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utIxWy_uiJ8Z",
        "colab_type": "code",
        "outputId": "20c6685a-0baa-46a9-d23c-b130807ac3c6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "# 파일을 콜랩에 업로드\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e577e065-5335-4469-9ec6-af2478e9c7f3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e577e065-5335-4469-9ec6-af2478e9c7f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dataset_except_flow_.csv to dataset_except_flow_.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRHlMVaS40Lp",
        "colab_type": "code",
        "outputId": "b72ada24-b293-43ca-e7fd-8173ab993e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "train = load_dataset('dataset_except_flow_.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjdYFZVeZw8F",
        "colab_type": "code",
        "outputId": "70060f48-d7f1-4a04-83ee-7679c453ebdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1907
        }
      },
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer</th>\n",
              "      <th>nextSentence</th>\n",
              "      <th>prevSentence</th>\n",
              "      <th>sentenceMax</th>\n",
              "      <th>sentenceNum</th>\n",
              "      <th>tf</th>\n",
              "      <th>type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The problem with\\r\\nthis is that the direct ap...</td>\n",
              "      <td>Roman authors could, in fact, make money from ...</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Sincerely, Mark Foster</td>\n",
              "      <td>Both eye and camera have a light-sensitive\\r\\n...</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>We welcome donations of your gently-used items</td>\n",
              "      <td>The\\r\\nplain old telephone was interactive, bu...</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>The evolutionary benefits of immediately getti...</td>\n",
              "      <td>If you spot a rhinoceros or hear an avalanche ...</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>The problem with\\r\\nthis is that the direct ap...</td>\n",
              "      <td>It is usually a win-win</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>The evolutionary benefits of immediately getti...</td>\n",
              "      <td>Sipping coffee leisurely at a caf?, Kate was e...</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>심경변화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>But how does someone listening to a story find...</td>\n",
              "      <td>Break out of this\\r\\npattern by paying more at...</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>As a schoolboy he\\r\\ndropped out of several sc...</td>\n",
              "      <td>Whatever he tried would\\r\\nstart out well and ...</td>\n",
              "      <td>23</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>High-tech clothing and\\r\\ndigital watches were...</td>\n",
              "      <td>When this happens, the\\r\\nweakest individuals ...</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Wintergreen leaves are not &amp;&amp; to the\\r\\nArctic...</td>\n",
              "      <td>Here’s an interesting thought</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>의미</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>The meaning of buildings evolves and becomes e...</td>\n",
              "      <td>Indeed, large numbers have been found to lack ...</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5</td>\n",
              "      <td>on June 12, 2017 ?Registration (online only): ...</td>\n",
              "      <td>Your mind will\\r\\nbecome more creative during ...</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5</td>\n",
              "      <td>We can discern different colors, but\\r\\nwe can...</td>\n",
              "      <td>Joachim-Ernst Berendt points out that the ear\\...</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>낱말</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>It is not hunted or eaten by any other animals...</td>\n",
              "      <td>When the day arrived, Chris was welcomed by th...</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>We hope this information makes your donation e...</td>\n",
              "      <td>They may show profit on the balance sheets of ...</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>요지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>This learning by doing is essential in many of...</td>\n",
              "      <td>The\\r\\nbaseball research shows us that what ma...</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>As a general rule, dominants tend to ignore\\r\\...</td>\n",
              "      <td>Within the past few years, new Internet-based ...</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>요지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>However, from a human point of view,\\r\\nobject...</td>\n",
              "      <td>The importance of science has led people to th...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>4</td>\n",
              "      <td>Totems are more than objects</td>\n",
              "      <td>This view of the earth and its riches as an in...</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>And what effect did all this might have on Ala...</td>\n",
              "      <td>The quake devastated 24,000 square miles of wi...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>의미</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>Bursting with happiness, I spent the rest of t...</td>\n",
              "      <td>” (e) He laughed and wiped away the tear stain...</td>\n",
              "      <td>26</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>일치</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1</td>\n",
              "      <td>But they often express themselves in this mann...</td>\n",
              "      <td>They often want to\\r\\nconvey their sense of ur...</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2</td>\n",
              "      <td>In fact, however, the effects\\r\\nof art are ne...</td>\n",
              "      <td>If the chimp pointed to the\\r\\nplate having mo...</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>5</td>\n",
              "      <td>The choice is up to you,\\r\\nso why not imitate...</td>\n",
              "      <td>You can’t change your genes, but you can\\r\\nch...</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "      <td>The former disagree more than the latter and s...</td>\n",
              "      <td>Not all children of successful\\r\\npeople becom...</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>3</td>\n",
              "      <td>Lake trout, for instance, stop\\r\\nbreeding as ...</td>\n",
              "      <td>Thus, reliable intermediary services will\\r\\nb...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5</td>\n",
              "      <td>Scientists were able to study the brain activi...</td>\n",
              "      <td>Good news for pet shop owners!\\r\\nWe, ABLE Shi...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>목적</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1</td>\n",
              "      <td>Look at how much\\r\\ncommunication happens thro...</td>\n",
              "      <td>Writing is making a\\r\\ncomeback all over our s...</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>4</td>\n",
              "      <td>One more try!” Taking a deep breath, he picked...</td>\n",
              "      <td>He\\r\\ntold me that when he started out in life...</td>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>Clues to past environmental change are well pr...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421</th>\n",
              "      <td>1</td>\n",
              "      <td>This might seem surprising, for the early cott...</td>\n",
              "      <td>A herd of zebras can become\\r\\na dazzling disp...</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2422</th>\n",
              "      <td>4</td>\n",
              "      <td>After their initial shock, they gather their r...</td>\n",
              "      <td>People who know this not only look for some po...</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>요지</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2423</th>\n",
              "      <td>5</td>\n",
              "      <td>They are not in the habit of observing closely...</td>\n",
              "      <td>Why does the “pure” acting of the movies not s...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>낱말</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2424</th>\n",
              "      <td>3</td>\n",
              "      <td>Certain animals and plants have a built-in sen...</td>\n",
              "      <td>This is known as a die-off</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2425</th>\n",
              "      <td>2</td>\n",
              "      <td></td>\n",
              "      <td>Seasonal variation thus prevents either group ...</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2426</th>\n",
              "      <td>1</td>\n",
              "      <td>?Badminton rackets and shuttlecocks are provided</td>\n",
              "      <td>Breaden, her only child, had always been the f...</td>\n",
              "      <td>13</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>심경변화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2427</th>\n",
              "      <td>1</td>\n",
              "      <td>After the performance, the three refused to pa...</td>\n",
              "      <td>Any\\r\\nhope of becoming a customer of that ide...</td>\n",
              "      <td>17</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>4</td>\n",
              "      <td>When challenged, the mind says (to itself),\\r\\...</td>\n",
              "      <td>When we behave irrationally, our behavior usua...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2429</th>\n",
              "      <td>1</td>\n",
              "      <td>This contrasts with the national view of the v...</td>\n",
              "      <td>Compensation in the form of planting on terrac...</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>요약</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>4</td>\n",
              "      <td>While my sisters were delighted to find their ...</td>\n",
              "      <td>A few, unable to\\r\\nsubsidize the property, wi...</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>낱말</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2431</th>\n",
              "      <td>2</td>\n",
              "      <td>There is evidence that groups with an even num...</td>\n",
              "      <td>At some point, these days, a parent realizes h...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2432</th>\n",
              "      <td>3</td>\n",
              "      <td>In other words, scientists thoroughly\\r\\nmisun...</td>\n",
              "      <td>The beta carotene supplement actually increase...</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2433</th>\n",
              "      <td>5</td>\n",
              "      <td>The individual’s participation in mass behavio...</td>\n",
              "      <td>In 1922, he established the Barnes Foundation ...</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>일치</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2434</th>\n",
              "      <td>4</td>\n",
              "      <td>“When we fantasize about it ― especially when\\...</td>\n",
              "      <td>To assess\\r\\nsubjects’ real life experiences, ...</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2435</th>\n",
              "      <td>2</td>\n",
              "      <td>If we think of tempo as the speed of events, t...</td>\n",
              "      <td>They are not deeply\\r\\nconcerned about how oth...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2436</th>\n",
              "      <td>2</td>\n",
              "      <td>Jot down notes and carry\\r\\nthem around in you...</td>\n",
              "      <td>We don’t infer that all persons with red hair\\...</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2437</th>\n",
              "      <td>4</td>\n",
              "      <td>Children raised in households that &amp;&amp;\\r\\ncommu...</td>\n",
              "      <td>Good parents listen to their children and\\r\\na...</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2438</th>\n",
              "      <td>1</td>\n",
              "      <td>A street in Anchorage fell twenty feet</td>\n",
              "      <td>Whether such women are American or Iranian or ...</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2439</th>\n",
              "      <td>2</td>\n",
              "      <td>Bursting with happiness, I spent the rest of t...</td>\n",
              "      <td>” (e) He laughed and wiped away the tear stain...</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>분위기</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2440</th>\n",
              "      <td>5</td>\n",
              "      <td>Trees do indeed have a few small roots which p...</td>\n",
              "      <td>The two plant types are therefore able to coex...</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2441</th>\n",
              "      <td>2</td>\n",
              "      <td>The repertoire varied according to the perform...</td>\n",
              "      <td>In essence, concert halls and opera houses had...</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2442</th>\n",
              "      <td>4</td>\n",
              "      <td>Sequoyah moved to Oklahoma some years later an...</td>\n",
              "      <td>One outcome of Enron’s subsequent financial co...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2443</th>\n",
              "      <td>2</td>\n",
              "      <td>They were wrong</td>\n",
              "      <td>The neural machinery for creating and holding ...</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2444</th>\n",
              "      <td>5</td>\n",
              "      <td>You have to be equally skilled at both</td>\n",
              "      <td>As\\r\\ncommunities became larger, some people h...</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2445</th>\n",
              "      <td>5</td>\n",
              "      <td>Why don’t you put your house number where it c...</td>\n",
              "      <td>Obvious examples\\r\\nwould be the use of unifor...</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>주제</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2446</th>\n",
              "      <td>4</td>\n",
              "      <td>“Wow!” he exclaimed</td>\n",
              "      <td>- All yodels should be sung either in French o...</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>일치</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2447</th>\n",
              "      <td>1</td>\n",
              "      <td>The concept of intellectual property continued...</td>\n",
              "      <td>Similar marks and symbols functioning as trade...</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2448</th>\n",
              "      <td>3</td>\n",
              "      <td>When one species eliminates another by outcomp...</td>\n",
              "      <td>Then, the teacher added a twist</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>제목</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2449</th>\n",
              "      <td>2</td>\n",
              "      <td>Thus, while climate change may not impact thes...</td>\n",
              "      <td>&amp;&amp;,\\r\\nclimate change may not only force the s...</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>빈칸</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2450</th>\n",
              "      <td>5</td>\n",
              "      <td>The primary motivation is the preservation of ...</td>\n",
              "      <td>People are much more concerned about catching ...</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>요지</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2451 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      answer                                       nextSentence  ... tf  type\n",
              "0          1  The problem with\\r\\nthis is that the direct ap...  ...  0    빈칸\n",
              "1          2                             Sincerely, Mark Foster  ...  0    빈칸\n",
              "2          1     We welcome donations of your gently-used items  ...  0    빈칸\n",
              "3          1  The evolutionary benefits of immediately getti...  ...  1    빈칸\n",
              "4          5  The problem with\\r\\nthis is that the direct ap...  ...  1    제목\n",
              "5          3  The evolutionary benefits of immediately getti...  ...  0  심경변화\n",
              "6          5  But how does someone listening to a story find...  ...  0    주제\n",
              "7          4  As a schoolboy he\\r\\ndropped out of several sc...  ...  1    제목\n",
              "8          1  High-tech clothing and\\r\\ndigital watches were...  ...  0    주제\n",
              "9          1  Wintergreen leaves are not && to the\\r\\nArctic...  ...  0    의미\n",
              "10         1  The meaning of buildings evolves and becomes e...  ...  0    제목\n",
              "11         5  on June 12, 2017 ?Registration (online only): ...  ...  0    주제\n",
              "12         5  We can discern different colors, but\\r\\nwe can...  ...  1    낱말\n",
              "13         1  It is not hunted or eaten by any other animals...  ...  0    빈칸\n",
              "14         5  We hope this information makes your donation e...  ...  0    요지\n",
              "15         2  This learning by doing is essential in many of...  ...  0    빈칸\n",
              "16         3  As a general rule, dominants tend to ignore\\r\\...  ...  0    요지\n",
              "17         1  However, from a human point of view,\\r\\nobject...  ...  1    빈칸\n",
              "18         4                       Totems are more than objects  ...  1    빈칸\n",
              "19         1  And what effect did all this might have on Ala...  ...  1    의미\n",
              "20         3  Bursting with happiness, I spent the rest of t...  ...  1    일치\n",
              "21         1  But they often express themselves in this mann...  ...  1    제목\n",
              "22         2  In fact, however, the effects\\r\\nof art are ne...  ...  0    빈칸\n",
              "23         5  The choice is up to you,\\r\\nso why not imitate...  ...  1    제목\n",
              "24         5  The former disagree more than the latter and s...  ...  0    제목\n",
              "25         3  Lake trout, for instance, stop\\r\\nbreeding as ...  ...  0    제목\n",
              "26         5  Scientists were able to study the brain activi...  ...  0    목적\n",
              "27         1  Look at how much\\r\\ncommunication happens thro...  ...  1    제목\n",
              "28         4  One more try!” Taking a deep breath, he picked...  ...  0    제목\n",
              "29         1                                                     ...  1    빈칸\n",
              "...      ...                                                ...  ... ..   ...\n",
              "2421       1  This might seem surprising, for the early cott...  ...  0    주제\n",
              "2422       4  After their initial shock, they gather their r...  ...  1    요지\n",
              "2423       5  They are not in the habit of observing closely...  ...  1    낱말\n",
              "2424       3  Certain animals and plants have a built-in sen...  ...  1    빈칸\n",
              "2425       2                                                     ...  1    제목\n",
              "2426       1   ?Badminton rackets and shuttlecocks are provided  ...  0  심경변화\n",
              "2427       1  After the performance, the three refused to pa...  ...  0    빈칸\n",
              "2428       4  When challenged, the mind says (to itself),\\r\\...  ...  1    빈칸\n",
              "2429       1  This contrasts with the national view of the v...  ...  1    요약\n",
              "2430       4  While my sisters were delighted to find their ...  ...  0    낱말\n",
              "2431       2  There is evidence that groups with an even num...  ...  0    빈칸\n",
              "2432       3  In other words, scientists thoroughly\\r\\nmisun...  ...  1    주제\n",
              "2433       5  The individual’s participation in mass behavio...  ...  0    일치\n",
              "2434       4  “When we fantasize about it ― especially when\\...  ...  1    빈칸\n",
              "2435       2  If we think of tempo as the speed of events, t...  ...  0    빈칸\n",
              "2436       2  Jot down notes and carry\\r\\nthem around in you...  ...  0    빈칸\n",
              "2437       4  Children raised in households that &&\\r\\ncommu...  ...  1    제목\n",
              "2438       1             A street in Anchorage fell twenty feet  ...  0    빈칸\n",
              "2439       2  Bursting with happiness, I spent the rest of t...  ...  1   분위기\n",
              "2440       5  Trees do indeed have a few small roots which p...  ...  1    제목\n",
              "2441       2  The repertoire varied according to the perform...  ...  1    제목\n",
              "2442       4  Sequoyah moved to Oklahoma some years later an...  ...  0    제목\n",
              "2443       2                                    They were wrong  ...  0    빈칸\n",
              "2444       5             You have to be equally skilled at both  ...  0    빈칸\n",
              "2445       5  Why don’t you put your house number where it c...  ...  0    주제\n",
              "2446       4                                “Wow!” he exclaimed  ...  0    일치\n",
              "2447       1  The concept of intellectual property continued...  ...  1    제목\n",
              "2448       3  When one species eliminates another by outcomp...  ...  0    제목\n",
              "2449       2  Thus, while climate change may not impact thes...  ...  1    빈칸\n",
              "2450       5  The primary motivation is the preservation of ...  ...  0    요지\n",
              "\n",
              "[2451 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciSZyc0934CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN_A = 'prevSentence'\n",
        "DATA_COLUMN_B = 'nextSentence'\n",
        "LABEL_COLUMN  = 'tf'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "label_list = [0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqjKz1WqeLU",
        "colab_type": "text"
      },
      "source": [
        "#### input 형태 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eo7dFK54c8q",
        "colab_type": "code",
        "outputId": "f916ed8b-903f-4a64-cd4a-898e0ab48234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
        "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                             text_a = x[DATA_COLUMN_A], \n",
        "                                                                             text_b = x[DATA_COLUMN_B], \n",
        "                                                                             label  = x[LABEL_COLUMN]),\n",
        "                                                                             axis = 1)\n",
        "\n",
        "'''\n",
        "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                           text_a = x[DATA_COLUMN_A], \n",
        "                                                                           text_b = x[DATA_COLUMN_B], \n",
        "                                                                           label  = x[LABEL_COLUMN]),\n",
        "                                                                           axis = 1)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \\n                                                                           text_a = x[DATA_COLUMN_A], \\n                                                                           text_b = x[DATA_COLUMN_B], \\n                                                                           label  = x[LABEL_COLUMN]),\\n                                                                           axis = 1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYpuzzDjqoQl",
        "colab_type": "text"
      },
      "source": [
        "#### BERT tokenizer import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHbyxVmeRRHO",
        "colab_type": "code",
        "outputId": "127e1b67-42d0-4174-d6a4-43eb86bf7a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "      \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0607 05:52:46.786335 140496214189952 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:49.129141 140496214189952 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW0U-_IEqtvw",
        "colab_type": "text"
      },
      "source": [
        "#### tokenizing → feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st9hgrgfRlpQ",
        "colab_type": "code",
        "outputId": "ed4b331e-4223-432b-a376-6ed0554ef41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1297
        }
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "#test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.229256 140496214189952 run_classifier.py:774] Writing example 0 of 2451\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.234340 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.236834 140496214189952 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] roman authors could , in fact , make money from the copying and publishing of their works because the value of their intellectual creations was recognized [SEP] the problem with this is that the direct approach can have an un ##int ##ended consequence : the loss of confidence [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.242545 140496214189952 run_classifier.py:464] tokens: [CLS] roman authors could , in fact , make money from the copying and publishing of their works because the value of their intellectual creations was recognized [SEP] the problem with this is that the direct approach can have an un ##int ##ended consequence : the loss of confidence [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3142 6048 2071 1010 1999 2755 1010 2191 2769 2013 1996 24731 1998 4640 1997 2037 2573 2138 1996 3643 1997 2037 7789 20677 2001 3858 102 1996 3291 2007 2023 2003 2008 1996 3622 3921 2064 2031 2019 4895 18447 21945 9509 1024 1996 3279 1997 7023 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.245113 140496214189952 run_classifier.py:465] input_ids: 101 3142 6048 2071 1010 1999 2755 1010 2191 2769 2013 1996 24731 1998 4640 1997 2037 2573 2138 1996 3643 1997 2037 7789 20677 2001 3858 102 1996 3291 2007 2023 2003 2008 1996 3622 3921 2064 2031 2019 4895 18447 21945 9509 1024 1996 3279 1997 7023 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.247366 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.249537 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.252124 140496214189952 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.255702 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.258601 140496214189952 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] both eye and camera have a light - sensitive layer onto which the image is cast ( the re ##tina and film , respectively ) [SEP] sincerely , mark foster [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.260771 140496214189952 run_classifier.py:464] tokens: [CLS] both eye and camera have a light - sensitive layer onto which the image is cast ( the re ##tina and film , respectively ) [SEP] sincerely , mark foster [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2119 3239 1998 4950 2031 1037 2422 1011 7591 6741 3031 2029 1996 3746 2003 3459 1006 1996 2128 13770 1998 2143 1010 4414 1007 102 25664 1010 2928 6469 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.263042 140496214189952 run_classifier.py:465] input_ids: 101 2119 3239 1998 4950 2031 1037 2422 1011 7591 6741 3031 2029 1996 3746 2003 3459 1006 1996 2128 13770 1998 2143 1010 4414 1007 102 25664 1010 2928 6469 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.265244 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.267911 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.270117 140496214189952 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.273859 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.276496 140496214189952 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the plain old telephone was interactive , but not integrated as it only transmitted speech and sounds and it did not work with digital code [SEP] we welcome donations of your gently - used items [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.278648 140496214189952 run_classifier.py:464] tokens: [CLS] the plain old telephone was interactive , but not integrated as it only transmitted speech and sounds and it did not work with digital code [SEP] we welcome donations of your gently - used items [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 5810 2214 7026 2001 9123 1010 2021 2025 6377 2004 2009 2069 11860 4613 1998 4165 1998 2009 2106 2025 2147 2007 3617 3642 102 2057 6160 11440 1997 2115 5251 1011 2109 5167 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.281221 140496214189952 run_classifier.py:465] input_ids: 101 1996 5810 2214 7026 2001 9123 1010 2021 2025 6377 2004 2009 2069 11860 4613 1998 4165 1998 2009 2106 2025 2147 2007 3617 3642 102 2057 6160 11440 1997 2115 5251 1011 2109 5167 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.283504 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.286214 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.288429 140496214189952 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.293610 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.296237 140496214189952 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] if you spot a rhino ##cer ##os or hear an avalanche speeding toward you , auditory looming will mo ##tiv ##ate you to jump out of the way now rather than wait until the last second [SEP] the evolutionary benefits of immediately getting out of the way of approaching dangers were so strong that natural selection endowed us ― and other mammals ― with brains that & & [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.298464 140496214189952 run_classifier.py:464] tokens: [CLS] if you spot a rhino ##cer ##os or hear an avalanche speeding toward you , auditory looming will mo ##tiv ##ate you to jump out of the way now rather than wait until the last second [SEP] the evolutionary benefits of immediately getting out of the way of approaching dangers were so strong that natural selection endowed us ― and other mammals ― with brains that & & [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2065 2017 3962 1037 24091 17119 2891 2030 2963 2019 18846 21485 2646 2017 1010 28042 23430 2097 9587 29068 3686 2017 2000 5376 2041 1997 1996 2126 2085 2738 2084 3524 2127 1996 2197 2117 102 1996 12761 6666 1997 3202 2893 2041 1997 1996 2126 1997 8455 16796 2020 2061 2844 2008 3019 4989 19038 2149 1518 1998 2060 11993 1518 2007 14332 2008 1004 1004 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.301187 140496214189952 run_classifier.py:465] input_ids: 101 2065 2017 3962 1037 24091 17119 2891 2030 2963 2019 18846 21485 2646 2017 1010 28042 23430 2097 9587 29068 3686 2017 2000 5376 2041 1997 1996 2126 2085 2738 2084 3524 2127 1996 2197 2117 102 1996 12761 6666 1997 3202 2893 2041 1997 1996 2126 1997 8455 16796 2020 2061 2844 2008 3019 4989 19038 2149 1518 1998 2060 11993 1518 2007 14332 2008 1004 1004 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.303435 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.305654 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.308293 140496214189952 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.311946 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.314552 140496214189952 run_classifier.py:462] guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] it is usually a win - win [SEP] the problem with this is that the direct approach can have an un ##int ##ended consequence : the loss of confidence [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.316714 140496214189952 run_classifier.py:464] tokens: [CLS] it is usually a win - win [SEP] the problem with this is that the direct approach can have an un ##int ##ended consequence : the loss of confidence [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2009 2003 2788 1037 2663 1011 2663 102 1996 3291 2007 2023 2003 2008 1996 3622 3921 2064 2031 2019 4895 18447 21945 9509 1024 1996 3279 1997 7023 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.319268 140496214189952 run_classifier.py:465] input_ids: 101 2009 2003 2788 1037 2663 1011 2663 102 1996 3291 2007 2023 2003 2008 1996 3622 3921 2064 2031 2019 4895 18447 21945 9509 1024 1996 3279 1997 7023 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.322306 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.324523 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:52:58.326647 140496214189952 run_classifier.py:468] label: 1 (id = 1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntZ-randIIz",
        "colab_type": "text"
      },
      "source": [
        "# 모델 정의 (Fine Tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Edai6KeS3qo",
        "colab_type": "text"
      },
      "source": [
        "#### create_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDLDWgEvdFs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels, \n",
        "                 num_labels):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  # 기본세팅\n",
        "  bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "  bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "  bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence. → [CLS] 토큰에 달리는 output?\n",
        "  # Use \"sequence_outputs\" for token-level output. → 몇 번째 time에 달지는 어떻게 설정?\n",
        "  # bert_outputs가 BERT의 기본 학습 출력값인 듯.\n",
        "  # bert_outputs[\"출력 종류\"] → 출력 종류는 \"pooled_output\" / \"sequence_outputs\" 두 가지 뿐? \n",
        "  output_layer = bert_outputs[\"pooled_output\"]\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own layer to tune for politeness data.\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels],\n",
        "      initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    # Dropout helps prevent overfitting\n",
        "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    # Convert labels into one-hot encoding\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "    \n",
        "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "    if is_predicting: # 입력으로 is_predicting 값을 1로 주면 predict까지 수행.\n",
        "      return (predicted_labels, log_probs)\n",
        "\n",
        "    # If we're train/eval, compute loss between predicted and actual label\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return (loss, predicted_labels, log_probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ufFskVS8iV",
        "colab_type": "text"
      },
      "source": [
        "#### model_fn_builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TSqwvcniEik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function using the passed parameters for num_labels, learning_rate, etc.\n",
        "\n",
        "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "    \n",
        "    # TRAIN and EVAL\n",
        "    if not is_predicting:\n",
        "\n",
        "      (loss, predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      train_op = bert.optimization.create_optimizer(\n",
        "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "      # Calculate evaluation metrics. \n",
        "      def metric_fn(label_ids, predicted_labels):\n",
        "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "        f1_score = tf.contrib.metrics.f1_score(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        auc = tf.metrics.auc(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        recall = tf.metrics.recall(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        precision = tf.metrics.precision(\n",
        "            label_ids,\n",
        "            predicted_labels) \n",
        "        true_pos = tf.metrics.true_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        true_neg = tf.metrics.true_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)   \n",
        "        false_pos = tf.metrics.false_positives(\n",
        "            label_ids,\n",
        "            predicted_labels)  \n",
        "        false_neg = tf.metrics.false_negatives(\n",
        "            label_ids,\n",
        "            predicted_labels)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"auc\": auc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"true_positives\": true_pos,\n",
        "            \"true_negatives\": true_neg,\n",
        "            \"false_positives\": false_pos,\n",
        "            \"false_negatives\": false_neg\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,\n",
        "          loss=loss,\n",
        "          train_op=train_op)\n",
        "      else:\n",
        "          return tf.estimator.EstimatorSpec(mode=mode,\n",
        "            loss=loss,\n",
        "            eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      (predicted_labels, log_probs) = create_model(\n",
        "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "      predictions = {\n",
        "          'probabilities': log_probs,\n",
        "          'labels': predicted_labels\n",
        "      }\n",
        "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "  # Return the actual model function in the closure\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgZrrh4tTQEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4-a_vgFTTZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRngFA6QTYBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwRuXpi8TgZa",
        "colab_type": "code",
        "outputId": "6a4c41dc-8fb1-4163-f613-83ebb4249440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "  model_fn=model_fn,\n",
        "  config=run_config,\n",
        "  params={\"batch_size\": BATCH_SIZE})\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bert_open_1/nextSentence', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc78b70ff28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:53:19.421385 140496214189952 estimator.py:201] Using config: {'_model_dir': 'gs://bert_open_1/nextSentence', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc78b70ff28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRBMBYWUTjFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ODv19kTm7u",
        "colab_type": "code",
        "outputId": "7912506d-0a73-4679-972f-f3f6d5ab2cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 05:53:26.490325 140496214189952 estimator.py:351] Skipping training since max_steps has already saved.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:00:02.014583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RMkDdT50sOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPrediction(sentences):\n",
        "  labels = [\"NotNext\", \"IsNext\"]\n",
        "  # in_sentences = [ [text_a,text_b], [text_a,text_b], [text_a,text_b], [text_a,text_b], ... ]\n",
        "  input_examples = [run_classifier.InputExample(guid=\"\", text_a = sentence[0], text_b = sentence[1], label = 0) for sentence in sentences] # here, \"\" is just a dummy label\n",
        "  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fWHgFBFaP5e",
        "colab_type": "code",
        "outputId": "b07e11b9-563d-485e-baa3-ec03f32be4f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "getPrediction([\"fuck you bro\",\"I really love you\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.160758 140496214189952 run_classifier.py:774] Writing example 0 of 2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.163968 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.167192 140496214189952 run_classifier.py:462] guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] f [SEP] u [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.169309 140496214189952 run_classifier.py:464] tokens: [CLS] f [SEP] u [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1042 102 1057 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.181207 140496214189952 run_classifier.py:465] input_ids: 101 1042 102 1057 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.185104 140496214189952 run_classifier.py:466] input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.188347 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.193478 140496214189952 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.198692 140496214189952 run_classifier.py:461] *** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.200748 140496214189952 run_classifier.py:462] guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.203107 140496214189952 run_classifier.py:464] tokens: [CLS] i [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.205693 140496214189952 run_classifier.py:465] input_ids: 101 1045 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.207782 140496214189952 run_classifier.py:466] input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.210166 140496214189952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.212171 140496214189952 run_classifier.py:468] label: 0 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:11.464707 140496214189952 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:15.409230 140496214189952 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:15.616760 140496214189952 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:16.207071 140496214189952 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from gs://bert_open_1/nextSentence/model.ckpt-229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:16.306324 140496214189952 saver.py:1270] Restoring parameters from gs://bert_open_1/nextSentence/model.ckpt-229\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:50.623863 140496214189952 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0607 06:09:50.771056 140496214189952 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fuck you bro', array([-5.039356  , -0.00649899], dtype=float32), 'IsNext'),\n",
              " ('I really love you',\n",
              "  array([-4.933206  , -0.00722948], dtype=float32),\n",
              "  'IsNext')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}